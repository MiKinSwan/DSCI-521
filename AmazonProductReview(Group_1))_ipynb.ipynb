{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "KHJuCMPT6aFq",
   "metadata": {
    "id": "KHJuCMPT6aFq"
   },
   "source": [
    "# **Sentiment Analysis of Amazon Product Reviews: All Beauty Category**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LwDdAUut7Lka",
   "metadata": {
    "id": "LwDdAUut7Lka"
   },
   "source": [
    "## Problem Statement\n",
    "Manual analysis of thousands of customer reviews is both slow and prone to human bias, so in this project we leverage NLP libraries to automate sentiment classification. We specifically choose on beauty products because they are an everyday essential and user opinions tend to be highly subjective.Therefore, we concentrate on Amazonâ€™s Beauty category to help shoppers make more informed choices. Although star ratings offer a rough performance signal, they miss the nuanced feedback found in the free-text reviews themselves. We compare three off-the-shelf sentiment scorers (star-rating mapping, TextBlob, VADER) and train five classifiers (Logistic Regression, NaÃ¯ve Bayes, SVM, Random Forest, Decision Tree) on each resulting label set. By evaluating accuracy and F1-score, we identify the most reliable approachâ€”one that can deliver fast, consistent insights to product managers so they can pinpoint key pain points, guide feature improvements, and ultimately boost customer satisfaction and sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4NtXyrvwXrSp",
   "metadata": {
    "id": "4NtXyrvwXrSp"
   },
   "source": [
    "## What is Sentiment Analysis? Why it matters?\n",
    "Sentiment analysis is a text-classification technique that automatically determines whether a piece of writing expresses positive, negative, or neutral sentiment. As customers share their thoughts more openly than ever in product reviews, surveys, or social mediaâ€”manually reading thousands of comments is impractical and prone to bias. By converting free-form language into structured sentiment scores at scale, organizations can quickly surface emerging product issues, identify high-impact features, and tailor products and services to meet customer needs. In e-commerce, sentiment analysis complements star ratings by capturing the nuance of what customers actually say, making it indispensable for data-driven decisions in product development, marketing, and customer-experience management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WsuIA1ujYSMw",
   "metadata": {
    "id": "WsuIA1ujYSMw"
   },
   "source": [
    "##Objectives of Project\n",
    "\n",
    "\n",
    "*   Data Collection\n",
    "\n",
    "*   Preprocessing and Cleaning\n",
    "\n",
    "*   Exploratory Data Analysis\n",
    "*   Generate Sentiment Labels via Multiple Strategies\n",
    "\n",
    "\n",
    "*   Extract Numerical Features\n",
    "\n",
    "\n",
    "*   Train and Compare Five Classifiers\n",
    "\n",
    "*   Evaluate Model Performance\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2CLrAn9g7DP-",
   "metadata": {
    "id": "2CLrAn9g7DP-"
   },
   "source": [
    "## Project Overview\n",
    "\n",
    "Customer reviews on e-commerce platforms such as Amazon are a valuable source of insights into customer satisfaction, product quality, and overall brand perception. This project focuses on sentiment analysis of customer reviews in the 'All Beauty' category on Amazon.\n",
    "\n",
    "\n",
    "#### Module submission group\n",
    "- Group member 1: Aishwarya Shastry Viswanath \n",
    "- Group member 2: Mi Kin Swan \n",
    "- Group member 3: Fatimah Aljohani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zJk9D-vb8jjh",
   "metadata": {
    "id": "zJk9D-vb8jjh"
   },
   "outputs": [],
   "source": [
    "# Import core data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import libraries for text processing and regular expressions\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Import progress bar and word cloud generation tools\n",
    "from tqdm.notebook import tqdm\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VMWqiwIp8NMI",
   "metadata": {
    "id": "VMWqiwIp8NMI"
   },
   "source": [
    "# Data Collection( swan)\n",
    "The dataset was sourced from the [McAuley Lab Amazon Reviews 2023](https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023) collection via Hugging Face. Using the Hugging Face Datasets library, we loaded the â€œall_beautyâ€ category of both User Reviews and Item Metadata directly into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m53Z9CkdmFh-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m53Z9CkdmFh-",
    "outputId": "0296d524-7438-4747-948c-6ec2c9b941a8"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to access datasets stored in Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B39jJF6kujS7",
   "metadata": {
    "id": "B39jJF6kujS7"
   },
   "outputs": [],
   "source": [
    "# Specify the file path to the Amazon beauty reviews CSV file stored in Google Drive\n",
    "file_path = '/content/drive/MyDrive/PROJECT AMAZON/raw_review_All_Beauty.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83aa87c-db13-4e25-abe7-0cd72b46fca7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a83aa87c-db13-4e25-abe7-0cd72b46fca7",
    "outputId": "6eb888bb-88bd-4a33-9af0-225cad25c0d5"
   },
   "outputs": [],
   "source": [
    "# Install the Hugging Face 'datasets' library to access the metadata\n",
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kPqlz0wraBJL",
   "metadata": {
    "id": "kPqlz0wraBJL"
   },
   "source": [
    "ðŸ”½ First, we loaded the User Reviews dataset, which contains 701,528 rows and 10 columns, and saved it as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27fe2f-9329-418d-88aa-5b44f7d7c32c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87,
     "referenced_widgets": [
      "696b164107a8493e996eb148e2a602b8",
      "5a41c4d2b07547ec9b5d3c68f09b42d6",
      "8ce782328a074c0d85b81ea4e82f5afc",
      "bd5f9b1f46234f47bb9d819e099b585c",
      "07c1bbe359e94a39b8e3d522ab4b5246",
      "7cada4b069894cc6a6babe2498982617",
      "7f28e4f129fc4bfea5bf00a650caa6dd",
      "5295749466884f54bba79ce70c5d8fda",
      "06dc55e5716d43eba7081805113b336f",
      "c90ebda11d8b4a0bbb96936e5b02a101",
      "935942bd40e44057865e004e1a94f198"
     ]
    },
    "id": "0c27fe2f-9329-418d-88aa-5b44f7d7c32c",
    "outputId": "54f551f7-a60a-4690-81cf-696a63fba527"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the All_Beauty reviews\n",
    "dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_All_Beauty\", trust_remote_code=True)\n",
    "\n",
    "# Access the full split\n",
    "reviews = dataset[\"full\"]\n",
    "\n",
    "# Display the first review\n",
    "print(reviews[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3be715-4ee7-4a94-a025-3e3bb0ce0f75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e3be715-4ee7-4a94-a025-3e3bb0ce0f75",
    "outputId": "2c297048-a455-4135-8847-605eb1da2925"
   },
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3046194-4433-4ceb-a567-cb4cacc6e59d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3046194-4433-4ceb-a567-cb4cacc6e59d",
    "outputId": "8b025f73-d617-47b7-81cf-467c71b01238"
   },
   "outputs": [],
   "source": [
    "# Show the first 10 rows\n",
    "for i in range(10):\n",
    "    print(reviews[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0987668-a6de-4996-973f-68771537495c",
   "metadata": {
    "id": "e0987668-a6de-4996-973f-68771537495c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b713862-f475-40b8-b1d3-a4538198c41d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "0b713862-f475-40b8-b1d3-a4538198c41d",
    "outputId": "a618a059-4857-404f-bbef-1395364ac7e7"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590efa9-fb9e-40d3-a406-f31e2b322088",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c590efa9-fb9e-40d3-a406-f31e2b322088",
    "outputId": "b840d27a-b4eb-4c65-a4da-c845feb4eaaf"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249874f-7233-4993-9e1d-2f15ccb46747",
   "metadata": {
    "id": "d249874f-7233-4993-9e1d-2f15ccb46747"
   },
   "outputs": [],
   "source": [
    "df.to_csv('raw_review_All_Beauty.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hl9iJkO1dLSl",
   "metadata": {
    "id": "hl9iJkO1dLSl"
   },
   "source": [
    "ðŸ”½ Second, we loaded the Item Metadata dataset, which contains 112,590 rows and 14 columns, and saved it as a JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6beb534-9b0a-44c6-b886-443fdd0c938d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6beb534-9b0a-44c6-b886-443fdd0c938d",
    "outputId": "36d09fba-4f22-4b22-d9c1-2d9bce69b0e6"
   },
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b05ca-fc67-4474-9b50-ac5eedcd3077",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "555b05ca-fc67-4474-9b50-ac5eedcd3077",
    "outputId": "98ddab66-8b7e-4d11-a535-fc006d8a4508"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the metadata file\n",
    "url = \"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/meta_categories/meta_All_Beauty.jsonl\"\n",
    "\n",
    "# Local file name to save\n",
    "output_file = \"meta_All_Beauty.jsonl\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    print(f\"Download complete. File saved as {output_file}\")\n",
    "else:\n",
    "    print(f\"Failed to download file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b9d5e-10f9-4f22-824d-8dc44a58f8c4",
   "metadata": {
    "id": "443b9d5e-10f9-4f22-824d-8dc44a58f8c4"
   },
   "outputs": [],
   "source": [
    "df_meta = pd.read_json(\"/content/drive/MyDrive/PROJECT AMAZON/meta_All_Beauty.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5387e-e192-4cd5-922b-62404a8d55e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "60d5387e-e192-4cd5-922b-62404a8d55e7",
    "outputId": "af1fe0da-99ee-4b9d-ce21-8299fe5ada6b"
   },
   "outputs": [],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HksMskDz-5CD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "HksMskDz-5CD",
    "outputId": "e0bd5dfb-e4b9-429b-a29f-dba8297e6df3"
   },
   "outputs": [],
   "source": [
    "!curl -L -o meta_All_Beauty.jsonl \"https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/resolve/main/raw/meta_categories/meta_All_Beauty.jsonl\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_meta = pd.read_json(\"meta_All_Beauty.jsonl\", lines=True)\n",
    "df_meta.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5vWXPr6S91A4",
   "metadata": {
    "id": "5vWXPr6S91A4"
   },
   "source": [
    "# Reading and Cleaning the data\n",
    "ðŸ”½ After that, we read both the metadata and review files and combined them into a single dataset before beginning preprocessing and cleaning on our selected features for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Hqeo_7j9yeF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "2Hqeo_7j9yeF",
    "outputId": "0c88fba8-d7b0-4780-868b-577064f35e6a"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "clean_data = []\n",
    "\n",
    "with open(\"/content/drive/MyDrive/DSCI-521/meta_All_Beauty.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            clean_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Skipping line {i} due to error: {e}\")\n",
    "            continue\n",
    "\n",
    "# converting json to datafram\n",
    "df_meta = pd.DataFrame(clean_data)\n",
    "\n",
    "\n",
    "df_meta.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xFLbKOcu_wFX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFLbKOcu_wFX",
    "outputId": "758563f6-2d0a-452d-9376-5fb751bb127b"
   },
   "outputs": [],
   "source": [
    "#check for the shape\n",
    "df_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ABhluSTS_zmQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABhluSTS_zmQ",
    "outputId": "a7c4cb07-2b0b-4d13-e981-7b7e348afce7"
   },
   "outputs": [],
   "source": [
    "#check for columns\n",
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vHY97vFnmA7l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "vHY97vFnmA7l",
    "outputId": "55d6bed5-60f0-4625-cdae-f9e4d4c9026d"
   },
   "outputs": [],
   "source": [
    "# reading reviews from google drive\n",
    "df_reviews = pd.read_csv(\"/content/drive/MyDrive/DSCI-521/raw_review_All_Beauty.csv\")\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rlm7j6S1_s8B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rlm7j6S1_s8B",
    "outputId": "44124956-6ac1-474e-a3ea-e9533c43738b"
   },
   "outputs": [],
   "source": [
    "#check for the shape\n",
    "df_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "APiXUzKk_tI8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "APiXUzKk_tI8",
    "outputId": "c1cf0c62-9ee1-4e8b-900a-d9e14458bed7"
   },
   "outputs": [],
   "source": [
    "#check for columns\n",
    "df_reviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3XCh6log_G8D",
   "metadata": {
    "id": "3XCh6log_G8D"
   },
   "outputs": [],
   "source": [
    "#Merge with Product Metadata\n",
    "df_combined = pd.merge(df_reviews, df_meta, on=\"parent_asin\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8BSvmIsF_G-g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 672
    },
    "id": "8BSvmIsF_G-g",
    "outputId": "3655462f-5fd0-40a8-cfea-5c0f549e5092"
   },
   "outputs": [],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KH1akvpQ_HCL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KH1akvpQ_HCL",
    "outputId": "edc475fe-e582-472c-8e7d-76421b927b75"
   },
   "outputs": [],
   "source": [
    "#check for the shape\n",
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tEeVYtYTBGII",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tEeVYtYTBGII",
    "outputId": "dd9bae7b-6cd5-4474-962f-6cddc482476d"
   },
   "outputs": [],
   "source": [
    "#check for columns\n",
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iJlSh5e0B_kC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iJlSh5e0B_kC",
    "outputId": "d50395ed-b695-4222-886e-ad7668e8e12b"
   },
   "outputs": [],
   "source": [
    "# Display summary info\n",
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EVUTmYyaPGsp",
   "metadata": {
    "id": "EVUTmYyaPGsp"
   },
   "outputs": [],
   "source": [
    "# saving the combine dataset\n",
    "df_combined.to_csv('All_Beauty.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CujS2nATDTe_",
   "metadata": {
    "id": "CujS2nATDTe_"
   },
   "outputs": [],
   "source": [
    "# rename the duplicate columns\n",
    "df_combined = df_combined.rename(columns={\n",
    "    \"title_x\": \"review_title\",\n",
    "    \"title_y\": \"product_title\",\n",
    "    \"images_x\": \"review_images\",\n",
    "    \"images_y\": \"product_images\",\n",
    "    \"text\": \"review_text\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I5R8Q_XvDeN6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5R8Q_XvDeN6",
    "outputId": "4a6e5a19-0215-46cb-ac00-3b5a54357a64"
   },
   "outputs": [],
   "source": [
    "#check for the columns\n",
    "df_combined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_bdnH4vqO6b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "_bdnH4vqO6b0",
    "outputId": "8c0747a8-ec46-4189-c55a-998a09450adf"
   },
   "outputs": [],
   "source": [
    "#check for null value\n",
    "df_combined.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EA82fcfcHy3o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EA82fcfcHy3o",
    "outputId": "33ed4562-0573-4f35-c14c-34cde8fd77e1"
   },
   "outputs": [],
   "source": [
    "#Check how many rating star we have\n",
    "df_combined[\"rating\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OpSQD-btIRrd",
   "metadata": {
    "id": "OpSQD-btIRrd"
   },
   "outputs": [],
   "source": [
    "# convert rating in to int\n",
    "df_combined[\"rating\"] = df_combined[\"rating\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jl2IEl2YIqOk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl2IEl2YIqOk",
    "outputId": "90a687e9-5f30-4fe0-ded3-4273cb8cb636"
   },
   "outputs": [],
   "source": [
    "#after changing int check again\n",
    "df_combined[\"rating\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BN90FKtiwUP5",
   "metadata": {
    "id": "BN90FKtiwUP5"
   },
   "source": [
    "ðŸ”½ Moreover, we first examine the rating column by mapping 4â€“5 â†’ â€œpositive,â€ 3 â†’ â€œneutral,â€ and 1â€“2 â†’ â€œnegative.â€ We then visualize this distribution using a bar chart, box plot, and pie chart.The results show that the majority of ratings fall into the â€œpositiveâ€ category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kgUqtZWmI7_n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "kgUqtZWmI7_n",
    "outputId": "ed3d0f2b-e3d0-4548-e15f-0ba6884fe157"
   },
   "outputs": [],
   "source": [
    "# Define a function to map rating to sentiment\n",
    "def label_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return \"positive\"\n",
    "    elif rating <= 2:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Apply the sentiment function to create a new column\n",
    "df_combined[\"sentiment\"] = df_combined[\"rating\"].apply(label_sentiment)\n",
    "\n",
    "# Display the unique values to verify\n",
    "df_combined[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2F_IVIGxJm_e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "2F_IVIGxJm_e",
    "outputId": "9551c82a-83b6-4b7c-f148-cf503e1d2cf9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of reviews per sentiment category\n",
    "sentiment_counts = df_combined[\"sentiment\"].value_counts()\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sentiment_counts.plot(kind='bar', color=['green', 'red', 'gray'])\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Sentiment Distribution of Reviews\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wsaKkiXDR8tg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "wsaKkiXDR8tg",
    "outputId": "7a79ebcd-494d-469d-b40e-981280bc7726"
   },
   "outputs": [],
   "source": [
    "# explore the rating based in the price\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define custom colors for sentiment\n",
    "sentiment_palette = {\n",
    "    \"positive\": \"green\",\n",
    "    \"neutral\": \"gray\",\n",
    "    \"negative\": \"red\"\n",
    "}\n",
    "\n",
    "# Boxplot: Price distribution by sentiment\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x=\"sentiment\", y=\"price\", data=df_combined, palette=sentiment_palette)\n",
    "plt.title(\"Price Distribution by Sentiment\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YDNpZlkeR3Qb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "id": "YDNpZlkeR3Qb",
    "outputId": "9fae95c3-3775-474d-c80f-1e3951c1e4f8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_combined is already loaded\n",
    "\n",
    "# Calculate rating counts\n",
    "rating_counts = df_combined['rating'].value_counts()\n",
    "\n",
    "# Create pie chart\n",
    "plt.figure(figsize=(8, 8))  # Adjust size as needed\n",
    "plt.pie(rating_counts, labels=rating_counts.index, autopct='%1.1f%%',\n",
    "        startangle=90, colors=['lightgreen', 'green', 'lightblue', 'blue', 'orange'])\n",
    "plt.title('Distribution of Customer Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83ba2cc-2b3e-428c-bb41-54120e030959",
   "metadata": {
    "id": "d83ba2cc-2b3e-428c-bb41-54120e030959"
   },
   "source": [
    "ðŸ”½ Next, we examine how many products fall into each price range, which shows that the vast majority of beauty products in the dataset are low-priced, and only a few appear in higher price brackets. In other words, the price distribution is heavily right-skewed: most items cost relatively little, and only a small number are very expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DbtTCrW-SEEE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "DbtTCrW-SEEE",
    "outputId": "7f8607de-e5dd-45cd-b60d-062ccc24b9a3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.hist(df_combined['price'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Product Prices')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff660d-b4e5-4896-bb37-69f1d7b8f418",
   "metadata": {
    "id": "4dff660d-b4e5-4896-bb37-69f1d7b8f418"
   },
   "source": [
    "ðŸ”½ Next, we examine the main category column to see how many beauty-product categories we have. Unfortunately, it only shows two: â€œAll Beautyâ€ and â€œPremium Beauty.â€ Later, we will create our own subcategories to analyze how customer sentiment is distributed within each custom group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suYnEKXpSXB1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "suYnEKXpSXB1",
    "outputId": "832b1071-538f-4305-f974-057b52c53ea9"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if 'main_category' or 'categories' exists\n",
    "if 'main_category' in df_combined.columns:\n",
    "    top_categories = df_combined['main_category'].value_counts().head(10)\n",
    "elif 'categories' in df_combined.columns and isinstance(df_combined['categories'].iloc[0], list):\n",
    "    # Assuming 'categories' contains lists of categories, flatten and count\n",
    "    all_categories = [cat for sublist in df_combined['categories'].tolist() for cat in sublist]\n",
    "    top_categories = pd.Series(all_categories).value_counts().head(10)\n",
    "else:\n",
    "    print(\"Neither 'main_category' nor 'categories' column found with expected format.\")\n",
    "\n",
    "    # If neither of the above works, you might need to examine the DataFrame further\n",
    "    # to see if any column represents category information under a different name.\n",
    "    # Print the column names to check:\n",
    "    print(df_combined.columns)\n",
    "\n",
    "# Only proceed if top_categories is defined\n",
    "if 'top_categories' in locals():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=top_categories.index, y=top_categories.values, palette='viridis')\n",
    "    plt.title('Top 10 Product Categories')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Number of Products')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b57efc-73c8-4545-b68c-24338a04123b",
   "metadata": {
    "id": "37b57efc-73c8-4545-b68c-24338a04123b"
   },
   "source": [
    "ðŸ”½ Next, we examine the distribution of sentiment labels for verified versus unverified purchases. We observe that most reviews particularly positive onesâ€”come from verified purchasers, suggesting a strong link between genuine product experience and favorable feedback. In contrast, unverified purchases exhibit a relatively higher share of negative sentiment, which may indicate that reviewers without firsthand experience are more likely to submit critical or misleading feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2Ovl7g_yS9GX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "2Ovl7g_yS9GX",
    "outputId": "0be24e44-5093-4234-916c-93aeea621310"
   },
   "outputs": [],
   "source": [
    "# explore Sentiment count by Verified Purchase\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "# Map True/False to strings for the palette\n",
    "verified_palette = {True: \"green\", False: \"red\"}\n",
    "\n",
    "sns.countplot(x=\"sentiment\", hue=\"verified_purchase\", data=df_combined, palette=verified_palette)\n",
    "plt.title(\"Sentiment by Verified Purchase\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.legend(title=\"Verified Purchase\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb9341c-116f-4da5-9428-2b2ed8371944",
   "metadata": {
    "id": "9bb9341c-116f-4da5-9428-2b2ed8371944"
   },
   "source": [
    "ðŸ”½ Finally, we defined a function to clean text by removing stopwords, punctuation, and numbers while retaining only words tagged as NOUN, VERB, ADJ, or ADV. We applied this function to both review_title and review_text to create clean_title and clean_text, and then saved the resulting DataFrame as All_Beauty_cleaned.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "srYoyut0YZKu",
   "metadata": {
    "id": "srYoyut0YZKu"
   },
   "outputs": [],
   "source": [
    "from os import pipe\n",
    "def clean_text(df,text):\n",
    "  text = df_combined[text].fillna('').astype(str).to_list()\n",
    "  nlp = spacy.load(\"en_core_web_sm\")\n",
    "  clean_text = []\n",
    "  for doc in tqdm(nlp.pipe(text,batch_size= 1000,disable=[\"ner\", \"parser\"]), total=len(text)):\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "    important_words = {\"not\", \"no\", \"never\", \"very\", \"just\", \"really\",\"like\",\"empty\",\"ten\",\"must\",\"serious\",\"yes\"}\n",
    "    custom_stopwords = stop_words.difference(important_words)\n",
    "    tokens =[\n",
    "        word.lemma_.lower().strip()\n",
    "        for word in doc\n",
    "        if not word.is_punct and not word.is_space and word.text.lower() not in custom_stopwords and not word.like_num and word.pos_ in{\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "    ]\n",
    "    clean_text.append(\" \".join(tokens))\n",
    "  return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gr2KpPHIcj6C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c98e8dcc141344afa4aac32e4796299c",
      "7fe13dcd1c4e4167926afeaf5765c770",
      "7b4228cafa9f4c189291508db65461f7",
      "20d28c4c8afc4c4eba33f6fe31ceca86",
      "4e3a0f12973c4c6898c81029adcea0fd",
      "b9a76d86096e49ccbd1e82a1534642be",
      "5e16775bb94e4a9cbf2443ef07f88d34",
      "b3beacc9044f4fe8962b2e07b80587b3",
      "b2eb1efb952541c9827a75244e4ae4fc",
      "229c619f940141609e3b9733f1e710e4",
      "685fb04a9cbf483aa75b4335294a68f8"
     ]
    },
    "id": "gr2KpPHIcj6C",
    "outputId": "e1d860a6-fd2a-4241-d991-5047f65bdff3"
   },
   "outputs": [],
   "source": [
    "df_combined['clean_title'] = clean_text(df_combined,'review_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v_pfPERTjA7b",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 146,
     "referenced_widgets": [
      "8d7a45e89a4d4220ba9347fe598985d0"
     ]
    },
    "id": "v_pfPERTjA7b",
    "outputId": "79a84e10-1dee-435d-c3d1-3fec994d7b35"
   },
   "outputs": [],
   "source": [
    "df_combined['clean_text'] = clean_text(df_combined,'review_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xUhmSkEmpi_-",
   "metadata": {
    "id": "xUhmSkEmpi_-"
   },
   "outputs": [],
   "source": [
    " # saving the clean dataset\n",
    "\n",
    "df_combined.to_csv('All_Beauty_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uSkoN6Q2yynl",
   "metadata": {
    "id": "uSkoN6Q2yynl"
   },
   "source": [
    "### ðŸ“¥ Loading the Cleaned Dataset and Preparing Date Features \n",
    "\n",
    "In this section, we load the cleaned and preprocessed version of the Amazon Beauty reviews dataset. This dataset has undergone multiple cleaning steps such as lowercasing, removal of punctuation and stopwords, lemmatization, and creation of new fields for modeling.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§¹ Loading the Dataset\n",
    "\n",
    "We read the cleaned dataset `All_Beauty_cleaned.csv` from Google Drive using `pandas`. This dataset includes 27+ columns such as:\n",
    "- `review_title`, `review_text`, `rating`\n",
    "- Metadata like `asin`, `store`, `categories`, `details`\n",
    "- Cleaned text fields: `clean_title`, `clean_text`, and `clean_review`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yo0hn85PtLdV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "yo0hn85PtLdV",
    "outputId": "f877f203-3725-4cba-8c92-4a72c8fffbc3"
   },
   "outputs": [],
   "source": [
    "# reading the cleaned dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/DSCI-521/All_Beauty_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c19d2-6c3c-4c88-a7d5-1ff8a5727983",
   "metadata": {
    "id": "415c19d2-6c3c-4c88-a7d5-1ff8a5727983"
   },
   "source": [
    "The dataset contains 27 columns and includes new features such as:\n",
    "\n",
    "clean_title â€“ cleaned version of the review title\n",
    "clean_text â€“ cleaned version of the review body\n",
    "clean_review â€“ concatenation of cleaned title and text\n",
    "Metadata fields like store, categories, product_images, and videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd5491-1a51-4b38-9848-6e92a1ecac74",
   "metadata": {
    "id": "50cd5491-1a51-4b38-9848-6e92a1ecac74"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Timestamp Conversion & Date Features**\n",
    "\n",
    "The original `timestamp` column (in Unix milliseconds) was converted to a human-readable `review_date`. We also extracted the `review_year` and `review_month` to enable time-based analysis.\n",
    "\n",
    "\n",
    "We visualized the number of reviews per year to identify trends in customer feedback over time.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C0EAcNay2ht_",
   "metadata": {
    "id": "C0EAcNay2ht_"
   },
   "outputs": [],
   "source": [
    "# converting timestamp type to date\n",
    "\n",
    "df['review_date'] = pd.to_datetime(df['timestamp'], unit='s', errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186ea24-ff18-4d0a-8255-34cb74ac66db",
   "metadata": {
    "id": "9186ea24-ff18-4d0a-8255-34cb74ac66db"
   },
   "source": [
    "Then, we extract the year and month of each review to allow time-based analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927d3af-c642-49c1-b29b-3ac88d3fafb0",
   "metadata": {
    "id": "0927d3af-c642-49c1-b29b-3ac88d3fafb0"
   },
   "source": [
    "###  **Timestamp Conversion & Date Features**\n",
    "\n",
    "The original `timestamp` column (in Unix milliseconds) was converted to a human-readable `review_date`. We also extracted the `review_year` and `review_month` to enable time-based analysis.\n",
    "\n",
    "```\n",
    "\n",
    "We visualized the number of reviews per year to identify trends in customer feedback over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hpz0bT5C2kmp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpz0bT5C2kmp",
    "outputId": "a94711fc-046e-4db4-8c38-5e97d8cf665c"
   },
   "outputs": [],
   "source": [
    "# Convert timestamp from milliseconds to datetime\n",
    "df['review_date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "\n",
    "# Optional: extract year/month for analysis\n",
    "df['review_year'] = df['review_date'].dt.year\n",
    "df['review_month'] = df['review_date'].dt.month\n",
    "\n",
    "# Check the result\n",
    "print(df[['timestamp', 'review_date', 'review_year', 'review_month']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y74abkdp3gMu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "id": "Y74abkdp3gMu",
    "outputId": "b5d18f24-0b0a-4e0b-cf08-dc626155ea24"
   },
   "outputs": [],
   "source": [
    "# Plot number of reviews by year\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(data=df, x='review_year', order=sorted(df['review_year'].dropna().unique()))\n",
    "plt.title(\"Number of Reviews by Year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Number of Reviews\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XI2vrZcD3wJc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "XI2vrZcD3wJc",
    "outputId": "93d1ec05-aeb0-4738-946e-c1355dc1599b"
   },
   "outputs": [],
   "source": [
    "# Convert timestamp from milliseconds to datetime\n",
    "df['review_date'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "\n",
    "# Drop the original timestamp column if not needed\n",
    "df.drop('timestamp', axis=1, inplace=True)\n",
    "\n",
    "# Optional: Reorder columns to place 'review_date' earlier\n",
    "# Move 'review_date' to the 4th column (for example)\n",
    "cols = list(df.columns)\n",
    "cols.insert(3, cols.pop(cols.index('review_date')))\n",
    "df = df[cols]\n",
    "\n",
    "# Check a sample\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9d3764-0c27-4dc9-96e0-adafdb3acdf6",
   "metadata": {
    "id": "2d9d3764-0c27-4dc9-96e0-adafdb3acdf6"
   },
   "source": [
    "## Sample View\n",
    "Here's a snapshot of what the updated DataFrame looks like:\n",
    "\n",
    "review_date: Human-readable review date\n",
    "review_year & review_month: Useful for time-series or seasonal analysis\n",
    "Cleaned review content (clean_review) ready for NLP modeling\n",
    "This step ensures that we are working with a clean, enriched dataset that's structured for both exploratory analysis and machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xmEpvuG9l44G",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "id": "xmEpvuG9l44G",
    "outputId": "904d98ad-090a-412e-b51c-28b9ee8d44d5"
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3C7DW0YG3vca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 637
    },
    "id": "3C7DW0YG3vca",
    "outputId": "99802bb2-37a7-4ff8-99a9-586b50a96e45"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SuxHtP6mlt9o",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SuxHtP6mlt9o",
    "outputId": "65c03362-99cb-4bda-a1b8-66aa5446ceee"
   },
   "outputs": [],
   "source": [
    "df['categories'].fillna('[]', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230ffc1-81f4-4545-9d8d-f10467972b23",
   "metadata": {
    "id": "2230ffc1-81f4-4545-9d8d-f10467972b23"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sWeqASRPlRkN",
   "metadata": {
    "id": "sWeqASRPlRkN"
   },
   "outputs": [],
   "source": [
    "df['categories'] = df['categories'].apply(lambda x: eval(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c414869d-1993-4d2d-976a-239a93934510",
   "metadata": {
    "id": "c414869d-1993-4d2d-976a-239a93934510"
   },
   "source": [
    "### ðŸ› ï¸ **Feature Enrichment and Preparation**\n",
    "\n",
    "After loading the cleaned dataset, we performed additional data preparation and feature extraction steps to enrich the dataset for deeper analysis and model training.\n",
    "\n",
    "---\n",
    "#### ðŸ“‚ **Combining Cleaned Text**\n",
    "\n",
    "We created a new feature, `clean_review`, by concatenating `clean_title` and `clean_text`. This unified text field will serve as the input for text vectorization and sentiment modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GUNhYFxWuX4-",
   "metadata": {
    "id": "GUNhYFxWuX4-"
   },
   "outputs": [],
   "source": [
    "df['clean_review'] = df['clean_title'] + \" \" + df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sY23uXVkuuxz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "id": "sY23uXVkuuxz",
    "outputId": "c0cd74d5-dd6f-4310-c568-1bbfa8b4a8fb"
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-6V634-CmS7y",
   "metadata": {
    "id": "-6V634-CmS7y"
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df.drop_duplicates(subset='clean_text', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e859a5-e5e7-4739-8676-4afd4e21f5fa",
   "metadata": {
    "id": "e3e859a5-e5e7-4739-8676-4afd4e21f5fa"
   },
   "source": [
    "## Cleaning Up\n",
    "\n",
    "After the conversion, we dropped the original timestamp column and reorganized the columns to place review_date near the front for better visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Uqs8vfAkmS-p",
   "metadata": {
    "id": "Uqs8vfAkmS-p"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['review_images'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd78cd06-755a-43cc-8a3d-d030affd13d2",
   "metadata": {
    "id": "bd78cd06-755a-43cc-8a3d-d030affd13d2"
   },
   "source": [
    "\n",
    "#### **Extracting 'Hair Type' Information**\n",
    "\n",
    "From the newly created `details_dict`, we extracted the `Hair Type` attribute and created a new column. Missing or unknown values were labeled as `'Unknown'`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbddFkrDmTBY",
   "metadata": {
    "id": "bbddFkrDmTBY"
   },
   "outputs": [],
   "source": [
    "# Convert string dict to actual dict\n",
    "from ast import literal_eval\n",
    "df['details_dict'] = df['details'].apply(lambda x: literal_eval(x) if pd.notnull(x) else {})\n",
    "\n",
    "# Extract a sample feature\n",
    "df['Hair_Type'] = df['details_dict'].apply(lambda x: x.get('Hair Type', None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b04e1-83c0-4981-9232-1ba3e1efc29e",
   "metadata": {
    "id": "556b04e1-83c0-4981-9232-1ba3e1efc29e"
   },
   "source": [
    "\n",
    "\n",
    "#### ðŸ§¾ **Parsing the `details` Column**\n",
    "\n",
    "The `details` column contains product specifications in stringified dictionary format. To extract useful features (like Hair Type), we used Pythonâ€™s `literal_eval` to safely convert strings into dictionaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "okg3FFpjm2Vy",
   "metadata": {
    "id": "okg3FFpjm2Vy"
   },
   "outputs": [],
   "source": [
    "# Function to safely convert strings to dictionaries\n",
    "def safe_literal_eval(x):\n",
    "    try:\n",
    "        return literal_eval(x) if isinstance(x, str) and x not in ['', 'nan'] else {}\n",
    "    except (ValueError, SyntaxError):  # Handle malformed strings\n",
    "        return {}\n",
    "\n",
    "# Convert string dict to actual dict, handling errors\n",
    "df['details_dict'] = df['details'].apply(safe_literal_eval)\n",
    "\n",
    "# Extract 'Hair Type' feature, providing a default value\n",
    "df['Hair_Type'] = df['details_dict'].apply(lambda x: x.get('Hair Type', 'Unknown'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a04d0b-fe3d-4ab4-9806-2235ec3bfb90",
   "metadata": {
    "id": "b8a04d0b-fe3d-4ab4-9806-2235ec3bfb90"
   },
   "source": [
    "#### **Binning Helpfulness Votes**\n",
    "\n",
    "To better understand review usefulness, we categorized `helpful_vote` into bins using `pd.cut`. This `helpful_bin` column allows for grouped analysis of reviews by perceived helpfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kH9RCA7bmYvx",
   "metadata": {
    "id": "kH9RCA7bmYvx"
   },
   "outputs": [],
   "source": [
    "df['helpful_bin'] = pd.cut(df['helpful_vote'], bins=[-1, 0, 2, 10, 50, 1000], labels=['0', '1-2', '3-10', '11-50', '50+'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9951c39-16b6-4a72-9ced-b09a6704dbb8",
   "metadata": {
    "id": "c9951c39-16b6-4a72-9ced-b09a6704dbb8"
   },
   "source": [
    "\n",
    "#### ðŸ§¾ Sample View of Final Dataset\n",
    "\n",
    "After these transformations, our enriched dataset contains over **30 columns**, including:\n",
    "\n",
    "* `clean_review`: preprocessed text for NLP\n",
    "* `Hair_Type`: extracted from product metadata\n",
    "* `helpful_bin`: binned helpfulness scores\n",
    "* `review_date`, `review_year`, `review_month`: derived from timestamps\n",
    "\n",
    "These features provide a robust foundation for sentiment analysis, user behavior modeling, and product category analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d2b8c-d56d-4e11-b0f8-66efff8e35f4",
   "metadata": {
    "id": "839d2b8c-d56d-4e11-b0f8-66efff8e35f4"
   },
   "source": [
    "Certainly! Here's a **concise and cohesive description** combining all the key points from your feature extraction and enrichment section. You can paste this into a markdown cell in your notebook:\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ **Feature Enrichment and Preparation â€“ Summary**\n",
    "\n",
    "After loading the cleaned Amazon Beauty reviews dataset, we performed several enrichment steps to prepare it for sentiment analysis and modeling. We began by combining `clean_title` and `clean_text` into a unified `clean_review` column, which serves as the main input for NLP tasks. We then processed the `categories` column by safely converting stringified lists into actual Python lists. For the `details` columnâ€”containing structured product metadataâ€”we used a robust parsing function to convert string dictionaries into usable Python dictionaries and extracted specific attributes like `Hair Type`, labeling unknown values accordingly.\n",
    "\n",
    "To support analysis of review quality, we binned the `helpful_vote` count into a new categorical variable, `helpful_bin`, representing perceived helpfulness ranges (e.g., 0, 1â€“2, 3â€“10, etc.). We also converted the original Unix `timestamp` to human-readable `review_date` and extracted `review_year` and `review_month` for time-based analysis.\n",
    "\n",
    "These enhancements resulted in a richly structured dataset of over 30 columns, enabling advanced text modeling, sentiment classification, and user behavior insights.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JTvCuUnLmYzY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "JTvCuUnLmYzY",
    "outputId": "b493f386-4c16-4af5-cfee-1f8eeb6a0b9e"
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a542a06-f7b5-4551-b8bc-e33df32a6212",
   "metadata": {
    "id": "7a542a06-f7b5-4551-b8bc-e33df32a6212"
   },
   "source": [
    "\n",
    "\n",
    "### ðŸ§´ðŸ§¼ **Sub-Category Classification Based on Product Titles**\n",
    "\n",
    "To better understand the types of beauty products in our dataset, we created a new feature called `sub_category` that classifies each product based on keywords found in the `product_title`. This helps in segmenting the dataset for more targeted analysis such as sentiment trends within each product category.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ› ï¸ **Classification Process**\n",
    "\n",
    "We defined a custom function `extract_subcategory()` to scan each product title for relevant keywords and assign it to one of the following predefined sub-categories:\n",
    "\n",
    "* **Hair Care** (e.g., shampoo, conditioner, hair)\n",
    "* **Skin Care** (e.g., moisturizer, serum, face wash)\n",
    "* **Makeup** (e.g., lipstick, foundation, mascara)\n",
    "* **Fragrance** (e.g., perfume, cologne)\n",
    "* **Body Care** (e.g., body wash, soap)\n",
    "* **Nail Care** (e.g., nail polish, manicure)\n",
    "* **Sun Care** (e.g., sunscreen, SPF)\n",
    "* **Deodorant**\n",
    "* **Other** (when no matching keywords were found)\n",
    "* **Unknown** (if title was missing)\n",
    "\n",
    "We then applied this function to the dataset:\n",
    "\n",
    "```python\n",
    "df[\"sub_category\"] = df[\"product_title\"].apply(extract_subcategory)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“Š **Sub-Category Distribution**\n",
    "\n",
    "After classification, we used value counts and a bar plot to visualize how reviews are distributed across different sub-categories.\n",
    "\n",
    "```python\n",
    "sub_category_counts = df['sub_category'].value_counts()\n",
    "```\n",
    "This helped us understand which types of products dominate the dataset. For example:\n",
    "\n",
    "* **Hair Care** and **Other** were the most frequent sub-categories.\n",
    "* Niche categories like **Sun Care** and **Deodorant** had fewer reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oWKc5DeRnOu8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "oWKc5DeRnOu8",
    "outputId": "f3926d1e-77b0-4828-d42e-fec97be64231"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a function to extract sub-category from the product title\n",
    "def extract_subcategory(title):\n",
    "    if pd.isnull(title):\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Convert title to lowercase for easier keyword matching\n",
    "    title = title.lower()\n",
    "\n",
    "    # Classify based on presence of keywords in the title\n",
    "    if any(keyword in title for keyword in [\"shampoo\", \"conditioner\", \"hair\"]):\n",
    "        return \"Hair Care\"\n",
    "    elif any(keyword in title for keyword in [\"moisturizer\", \"serum\", \"cream\", \"lotion\", \"cleanser\", \"face wash\"]):\n",
    "        return \"Skin Care\"\n",
    "    elif any(keyword in title for keyword in [\"lipstick\", \"mascara\", \"eyeshadow\", \"foundation\", \"concealer\", \"blush\"]):\n",
    "        return \"Makeup\"\n",
    "    elif any(keyword in title for keyword in [\"perfume\", \"fragrance\", \"eau de\", \"cologne\"]):\n",
    "        return \"Fragrance\"\n",
    "    elif any(keyword in title for keyword in [\"body wash\", \"soap\", \"scrub\", \"bath\"]):\n",
    "        return \"Body Care\"\n",
    "    elif any(keyword in title for keyword in [\"nail\", \"polish\", \"manicure\", \"pedicure\"]):\n",
    "        return \"Nail Care\"\n",
    "    elif any(keyword in title for keyword in [\"sunscreen\", \"spf\", \"sunblock\"]):\n",
    "        return \"Sun Care\"\n",
    "    elif any(keyword in title for keyword in [\"deodorant\", \"antiperspirant\"]):\n",
    "        return \"Deodorant\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Apply the function to classify products based on their title\n",
    "df[\"sub_category\"] = df[\"product_title\"].apply(extract_subcategory)\n",
    "\n",
    "# Display the frequency of each sub-category\n",
    "df[\"sub_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DjoKDaJTS96i",
   "metadata": {
    "id": "DjoKDaJTS96i"
   },
   "source": [
    "\n",
    "We visualized the distribution with the following plot:\n",
    "\n",
    "\n",
    "#### âœ… **Outcome**\n",
    "\n",
    "This sub-category classification enables us to:\n",
    "\n",
    "* Perform more detailed exploratory data analysis\n",
    "* Compare sentiment scores or review patterns across categories\n",
    "* Use sub-category as an additional feature in predictive models\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like to convert this into a slide or summary table as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G3UBaGkJmY1Z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "G3UBaGkJmY1Z",
    "outputId": "5b3a6ac5-2b96-4de7-a465-c00b664177fa"
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sub_category_counts = df['sub_category'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=sub_category_counts.index, y=sub_category_counts.values, palette='viridis')\n",
    "plt.title('Sub-Category Distribution')\n",
    "plt.xlabel('Sub-Category')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee47df05-1fcb-4b34-b20f-ac172218cc2e",
   "metadata": {
    "id": "ee47df05-1fcb-4b34-b20f-ac172218cc2e"
   },
   "source": [
    "\n",
    "#### ðŸ“Š **Sub-Category Distribution**\n",
    "\n",
    "After classification, we used value counts and a bar plot to visualize how reviews are distributed across different sub-categories.\n",
    "\n",
    "\n",
    "This helped us understand which types of products dominate the dataset. For example:\n",
    "\n",
    "* **Hair Care** and **Other** were the most frequent sub-categories.\n",
    "* Niche categories like **Sun Care** and **Deodorant** had fewer reviews.\n",
    "\n",
    "We visualized the distribution with the following plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o7MyBsjsmY3f",
   "metadata": {
    "id": "o7MyBsjsmY3f"
   },
   "outputs": [],
   "source": [
    "df = df[~df['sub_category'].isin(['Unknown'])]  # Remove Unknowns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Lxdy-3LTmY5k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lxdy-3LTmY5k",
    "outputId": "9b4c43b7-ea23-4a71-ec3b-0fce3ec4b6f6"
   },
   "outputs": [],
   "source": [
    "small_cats = ['Fragrance', 'Deodorant', 'Sun Care']\n",
    "df['sub_category_grouped'] = df['sub_category'].apply(lambda x: x if x not in small_cats else 'Other')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vQrI_1cSUCjo",
   "metadata": {
    "id": "vQrI_1cSUCjo"
   },
   "source": [
    "### ðŸ“Š Sentiment Distribution & Time-Based Rating Trends\n",
    "\n",
    "In this section, we performed two key analyses:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1ï¸âƒ£ Sentiment Distribution by Product Sub-Category\n",
    "\n",
    "We created sentiment labels based on review ratings:\n",
    "- Ratings **â‰¥ 4** â†’ **Positive**\n",
    "- Ratings **= 3** â†’ **Neutral**\n",
    "- Ratings **â‰¤ 2** â†’ **Negative**\n",
    "\n",
    "Using this classification, we plotted the distribution of sentiments across different `sub_category` values (e.g., Hair Care, Skin Care, Makeup). This allowed us to see which product types had more balanced or polarized reviews.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2ï¸âƒ£ Time-Based Product Rating Trends\n",
    "\n",
    "We calculated the **change in product rating over time** to assess how customer perception has shifted. Using the `groupby().diff()` method, we computed a column `Delta` to reflect the difference in ratings spaced six entries apart (as a proxy for time progression).\n",
    "\n",
    "Focusing on the **Skin Care** sub-category, we identified the **Top 10 products** with the most improved ratings and visualized them in a bar chart. This highlighted which skincare products have gained favor among users over the years.\n",
    "\n",
    "These analyses help uncover:\n",
    "- Sentiment tendencies across product types\n",
    "- Long-term improvements in customer satisfaction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7G1IWKU18d6m",
   "metadata": {
    "id": "7G1IWKU18d6m"
   },
   "outputs": [],
   "source": [
    "# Create sentiment labels from ratings\n",
    "df['Sentiment'] = df['rating'].apply(lambda x: 'positive' if x >= 4 else 'negative' if x <= 2 else 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SvDYcUHImY7n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "SvDYcUHImY7n",
    "outputId": "34d1c799-cf0b-4704-c5df-2401685b863a"
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='sub_category', hue='Sentiment')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Sentiment Distribution per Subcategory')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UGEnENq2pCK0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "UGEnENq2pCK0",
    "outputId": "02ae3211-9579-4552-ace5-ee378adaf28b"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Calculate the change in rating over 6 years\n",
    "\n",
    "df['Delta'] = df.groupby('product_title')['rating'].diff(periods=6)  # Difference over 6 years\n",
    "\n",
    "# Filter for skincare products and select top 10 with highest improvement\n",
    "# Replace 'sub_category' with the column representing product category if it's not 'category'\n",
    "top_10_rise_skin = df[df['sub_category'] == 'Skin Care'].sort_values(by='Delta', ascending=False).head(10)\n",
    "\n",
    "# Create the barplot\n",
    "Top_10_plt = sns.barplot(x='product_title', y='Delta', data=top_10_rise_skin)  # Use the DataFrame directly\n",
    "Top_10_plt.set_title('Top 10 Skincare Products with Improved Rating in 6 Years', fontsize=15, y=1.05)\n",
    "Top_10_plt.set_xticklabels(Top_10_plt.get_xticklabels(), rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iXOdyhb1UULC",
   "metadata": {
    "id": "iXOdyhb1UULC"
   },
   "source": [
    "#### ðŸ—‚ï¸ Product Classification by Main Category\n",
    "\n",
    "We checked how reviews are distributed across the two main product categories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q1MdonBEdVdU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "q1MdonBEdVdU",
    "outputId": "0212bb64-1aa5-4d9a-90cc-1aeb473ffdc7"
   },
   "outputs": [],
   "source": [
    "# each product classification\n",
    "df['main_category'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oEIyppNeUeRC",
   "metadata": {
    "id": "oEIyppNeUeRC"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "#### Handling Missing Values\n",
    "\n",
    "We checked for missing values using\n",
    "\n",
    "Then we dropped rows with missing values in the `clean_review` or `Sentiment` columns as these are critical for sentiment modeling:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h4qOomdd4iFI",
   "metadata": {
    "id": "h4qOomdd4iFI"
   },
   "outputs": [],
   "source": [
    "# See nulls\n",
    "df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "# Drop rows with missing clean_review or sentiment\n",
    "df = df.dropna(subset=['clean_review', 'Sentiment'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OPi_j5GOVE6C",
   "metadata": {
    "id": "OPi_j5GOVE6C"
   },
   "source": [
    "\n",
    "#### ðŸ†• . Creating a Binary Helpfulness Feature\n",
    "\n",
    "We added a new feature `helpful` that marks a review as \"helpful\" (1) if it received at least one helpful vote:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XY75Q5s-4iH1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "XY75Q5s-4iH1",
    "outputId": "b7979bce-87cd-433e-ddbe-f869dd83639f"
   },
   "outputs": [],
   "source": [
    "# Create a new binary feature: was the review helpful or not\n",
    "df['helpful'] = df['helpful_vote'].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# You can also look at helpfulness by sentiment:\n",
    "helpful_summary = df.groupby('Sentiment')['helpful_vote'].mean()\n",
    "helpful_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vm72Br6qVtBc",
   "metadata": {
    "id": "vm72Br6qVtBc"
   },
   "source": [
    "### ðŸ“… Monthly Review Trends Over Time\n",
    "\n",
    "In this section, we analyzed how the volume of product reviews changed over time.\n",
    "\n",
    "\n",
    "####   Creating a Monthly Period Column\n",
    "\n",
    "We extracted the **month and year** from the `review_date` column using `dt.to_period('M')` and stored it in a new column `review_month`.Also We grouped the dataset by `review_month` and counted the number of reviews in each month:\n",
    "\n",
    "\n",
    "\n",
    "ðŸ“Œ **Insight:**\n",
    "This visualization helps us understand periods of high or low user engagement and whether review activity increased over specific years (e.g., spikes around 2020â€“2021).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nZOgbYvh40-J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "id": "nZOgbYvh40-J",
    "outputId": "e409eb9b-c74b-4705-fc33-49e5c180fc85"
   },
   "outputs": [],
   "source": [
    "# Monthly reviews over time\n",
    "df['review_month'] = df['review_date'].dt.to_period('M')\n",
    "monthly_reviews = df.groupby('review_month').size()\n",
    "\n",
    "monthly_reviews.plot(kind='line', figsize=(12, 6), title='Monthly Review Volume')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687zhmbp41Cw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "687zhmbp41Cw",
    "outputId": "58cad485-a8c0-4cdd-b2a3-6043113a97a5"
   },
   "outputs": [],
   "source": [
    "# Monthly reviews over time\n",
    "df['review_month'] = df['review_date'].dt.to_period('M')\n",
    "monthly_reviews = df.groupby('review_month').size()\n",
    "\n",
    "monthly_reviews.plot(kind='line', figsize=(12, 6), title='Monthly Review Volume')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xlabel('Month')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7DpiMz5hWwtD",
   "metadata": {
    "id": "7DpiMz5hWwtD"
   },
   "source": [
    "\n",
    "\n",
    "####. Verifying Product Categories\n",
    "\n",
    "We inspected the unique values in the `main_category` column:\n",
    "\n",
    "\n",
    "\n",
    "Filtered to only show products in the **All Beauty** category and verified the filtering\n",
    "This ensures that subsequent analyses (like time trends or sentiment by category) focus specifically on the dominant category in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SBs_5Ph1dVgj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBs_5Ph1dVgj",
    "outputId": "610a5bcf-b5d3-4f4b-e625-e193255373b3"
   },
   "outputs": [],
   "source": [
    "#list of all produtcs category\n",
    "df['main_category'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nK_jkasBeixe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nK_jkasBeixe",
    "outputId": "7a048695-c97d-43d0-e044-83892b5ba21e"
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to select rows where the 'main_category' column equals 'All Beauty'\n",
    "filtered_df = df[df['main_category'] == 'All Beauty']\n",
    "\n",
    "# Get the unique values in the 'main_category' column of the filtered DataFrame\n",
    "unique_categories = filtered_df['main_category'].unique()\n",
    "\n",
    "# Print the unique categories (should only be 'All Beauty' if the filtering worked)\n",
    "print(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ivKYS4mXeGRZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivKYS4mXeGRZ",
    "outputId": "7e9a434c-7da4-435f-dc4b-f50178e93e8b"
   },
   "outputs": [],
   "source": [
    "print(df['main_category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5ljhWj0fiPx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "y5ljhWj0fiPx",
    "outputId": "3f9b74b0-5388-4968-dd4c-6ab49db630a1"
   },
   "outputs": [],
   "source": [
    "# Show top 10 frequent words\n",
    "word_freq = word_count(df, 'clean_text')\n",
    "top_10_words = word_freq.most_common(10)\n",
    "top_10_words = pd.DataFrame(top_10_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "# Correct the column names in the sns.barplot call:\n",
    "ax = sns.barplot(data=top_10_words, x='Word', y='Frequency')  # Use 'Word' and 'Frequency'\n",
    "\n",
    "ax.set_title('Compound Score by Amazon Star Review')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gVrUGJT-vSl5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "gVrUGJT-vSl5",
    "outputId": "d5e6242f-2c3b-4a7f-9050-584445d0edc8"
   },
   "outputs": [],
   "source": [
    "#Word cloud of frequent words\n",
    "word_cloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(word_cloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Frequent Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zfqXbpV3XSzj",
   "metadata": {
    "id": "zfqXbpV3XSzj"
   },
   "source": [
    "### ðŸ“Š Sentiment Class Distribution\n",
    "\n",
    "To understand the balance of sentiment labels in our dataset, we visualized the distribution of `Sentiment` values using a bar plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nMiZvflu4ZPi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 563
    },
    "id": "nMiZvflu4ZPi",
    "outputId": "5ffe4a81-624f-439e-a259-48f57a93e241"
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=df, x='Sentiment')\n",
    "plt.title(\"Class Distribution: Positive vs Negative Reviews\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "# Check actual counts/ratios\n",
    "print(df['Sentiment'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3E7mnIMjXlM0",
   "metadata": {
    "id": "3E7mnIMjXlM0"
   },
   "source": [
    "## ðŸ” Exploratory Data Analysis (EDA) & Feature Engineering â€“ Summary\n",
    "\n",
    "In this section, we explored the dataset thoroughly and engineered meaningful features to prepare it for sentiment classification and modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š Exploratory Data Analysis (EDA)\n",
    "\n",
    "#### 1ï¸âƒ£ Dataset Overview\n",
    "- Loaded a cleaned dataset with ~600,000 Amazon product reviews in the **All Beauty** and **Premium Beauty** categories.\n",
    "- Key columns included: `rating`, `review_text`, `verified_purchase`, `helpful_vote`, `categories`, and product `details`.\n",
    "\n",
    "#### 2ï¸âƒ£ Temporal Analysis\n",
    "- Converted the Unix `timestamp` column into a human-readable `review_date`.\n",
    "- Extracted `review_year` and `review_month` for time-based trend analysis.\n",
    "- Visualized **monthly review volume** using a line plot to observe customer engagement over time.\n",
    "\n",
    "#### 3ï¸âƒ£ Rating Distribution\n",
    "- Plotted the distribution of `rating` values (1â€“5 stars).\n",
    "- Identified that the dataset is **skewed toward positive ratings** (4â€“5 stars).\n",
    "\n",
    "#### 4ï¸âƒ£ Sentiment Distribution\n",
    "- Created a new `Sentiment` column from ratings:\n",
    "  - 4â€“5 â†’ Positive  \n",
    "  - 3 â†’ Neutral  \n",
    "  - 1â€“2 â†’ Negative\n",
    "- Count plot showed class imbalance with ~70% positive reviews.\n",
    "- Calculated exact proportions using `value_counts(normalize=True)`.\n",
    "\n",
    "#### 5ï¸âƒ£ Sub-Category Classification\n",
    "- Created a new column `sub_category` by extracting product types (e.g., \"shampoo\" â†’ Hair Care).\n",
    "- Visualized the distribution of reviews across sub-categories using a bar plot.\n",
    "\n",
    "#### 6ï¸âƒ£ Product Metadata Exploration\n",
    "- Parsed the `details` column (stringified dictionary) using `literal_eval`.\n",
    "- Extracted meaningful product attributes like `Hair_Type`.\n",
    "\n",
    "#### 7ï¸âƒ£ Helpfulness Analysis\n",
    "- Created a binary feature `helpful` (1 if `helpful_vote` > 0).\n",
    "- Grouped by sentiment to calculate average helpfulness:\n",
    "  - Positive and negative reviews were more often marked helpful than neutral ones.\n",
    "\n",
    "#### 8ï¸âƒ£ Time-Based Product Rating Trends\n",
    "- Calculated `Delta` (change in rating) to track improvement over time.\n",
    "- Identified the **top 10 Skin Care products** with the highest increase in rating over a 6-year span.\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ Feature Engineering\n",
    "\n",
    "We created several new features to enhance the dataset and support effective model training:\n",
    "\n",
    "#### âœ… Text Features\n",
    "- `clean_title` and `clean_text`: Preprocessed versions of raw review content.\n",
    "- `clean_review`: Combined `clean_title` + `clean_text` as a unified input for sentiment modeling.\n",
    "\n",
    "#### ðŸ•“ Time Features\n",
    "- `review_year`, `review_month`: Extracted from converted `review_date`.\n",
    "\n",
    "#### ðŸ“¦ Categorical Features\n",
    "- `sub_category`: Product type (Hair Care, Makeup, etc.) derived from title keywords.\n",
    "- `Hair_Type`: Extracted from `details_dict`.\n",
    "\n",
    "#### ðŸ‘ Helpfulness Features\n",
    "- `helpful`: Binary feature based on `helpful_vote`.\n",
    "- `helpful_bin`: Grouped helpful vote ranges (e.g., 0, 1â€“2, 3â€“10, etc.)\n",
    "\n",
    "These enriched features laid the foundation for robust sentiment classification and deeper product insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A02OGEa_NIb7",
   "metadata": {
    "id": "A02OGEa_NIb7"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16eQ3AQOOaRk",
   "metadata": {
    "id": "16eQ3AQOOaRk"
   },
   "source": [
    "## ðŸ§  Sentiment Classification: Three Approaches\n",
    "\n",
    "In this section, we classified the sentiment of customer reviews into three categories:\n",
    "- **Positive**\n",
    "- **Negative**\n",
    "- **Neutral**\n",
    "\n",
    "We applied and compared three different sentiment classification techniques:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Manual Labeling Based on Star Ratings\n",
    "\n",
    "We first created sentiment labels using the `rating` column, which reflects the star rating provided by the customer (from 1 to 5).\n",
    "\n",
    "This method assumes that star ratings accurately reflect the customer's sentiment toward the product.\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Rule-Based Classification Using TextBlob\n",
    "\n",
    "We then applied the `TextBlob` library, which uses a built-in sentiment lexicon to compute a **polarity score** for each review and then it's automatically classified into a sentiment category based on its polarity score.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Rule-Based Classification Using VADER\n",
    "\n",
    "Finally, we used `VADER` (Valence Aware Dictionary and sEntiment Reasoner) from the NLTK library. It is especially effective for short texts and reviews. VADER calculates a **compound sentiment score**\n",
    "\n",
    "Like TextBlob, this method also assigns sentiment directly from the text content of the review.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Why Use Three Methods?\n",
    "\n",
    "Using multiple sentiment classification approaches allows us to:\n",
    "\n",
    "- Compare rule-based predictions (TextBlob & VADER) to the star-based ground truth\n",
    "- Explore whether textual sentiment aligns with user ratings\n",
    "- Identify inconsistencies or patterns between rating and review content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4-tuVxm_midu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-tuVxm_midu",
    "outputId": "7bafddf6-5e97-4a3e-cadd-53a6cb4501d7"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zFXvPfjSwGpm",
   "metadata": {
    "id": "zFXvPfjSwGpm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df= pd.read_csv('/content/drive/MyDrive/Amazon/All_Beauty_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FSRt9Fbrfing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FSRt9Fbrfing",
    "outputId": "713bea2b-d3c6-4351-f8d5-08b91fa88c28"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pTwxKPEQ_NzK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "pTwxKPEQ_NzK",
    "outputId": "b2ae442e-cb13-44e7-8699-e58f6d4a5dfe"
   },
   "outputs": [],
   "source": [
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tNYuxX7wOtY-",
   "metadata": {
    "id": "tNYuxX7wOtY-"
   },
   "source": [
    "### 1ï¸âƒ£ Manual Labeling Based on Star Ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MsrKfcCdQbSt",
   "metadata": {
    "id": "MsrKfcCdQbSt"
   },
   "source": [
    "To establish a baseline for sentiment classification, we manually labeled each review using the numerical star ratings provided by customers (from 1 to 5).\n",
    "\n",
    "We defined the sentiment classes as follows:\n",
    "\n",
    "- Ratings **1 or 2** â†’ **Negative (0)**\n",
    "- Rating **3** â†’ **Neutral (1)**\n",
    "- Ratings **4 or 5** â†’ **Positive (2)**\n",
    "\n",
    "This approach assumes that the star rating reflects the user's sentiment toward the product.\n",
    "\n",
    "We then created a new column called `label` in the dataset to store these sentiment categories.\n",
    "\n",
    "This manual labeling will be used as the **ground truth** when comparing results from rule-based models such as **TextBlob** and **VADER**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ehNsFgTaQSrt",
   "metadata": {
    "id": "ehNsFgTaQSrt"
   },
   "outputs": [],
   "source": [
    "# Convert rating column to numeric\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "\n",
    "# Keep only ratings 1 through 5 (drop any missing or invalid ratings)\n",
    "df = df[df['rating'].isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "# Create multi-class sentiment labels:\n",
    "# 0 = Negative, 1 = Neutral, 2 = Positive\n",
    "def label_sentiment(rating):\n",
    "    if rating in [1, 2]:\n",
    "        return 0\n",
    "    elif rating == 3:\n",
    "        return 1\n",
    "    else:  # rating 4 or 5\n",
    "        return 2\n",
    "\n",
    "df['label'] = df['rating'].apply(label_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iu8HZLvyfitg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "Iu8HZLvyfitg",
    "outputId": "a4984314-0b17-4009-f4c8-1bcce8663c36"
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5GWtGGmJfiwb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "5GWtGGmJfiwb",
    "outputId": "92b26ec7-1732-4252-ed23-958485d26b4d"
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CNsKmL74fizb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "id": "CNsKmL74fizb",
    "outputId": "adb95ce7-4f52-4ce0-f008-43ecbfdf8539"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "\n",
    "sns.countplot(x='label', data=df, palette='viridis')\n",
    "\n",
    "\n",
    "plt.title('Distribution of Sentiment Labels')\n",
    "plt.xlabel('Label (0 = Negative, 1 = Neutral, 2= Positive)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "\n",
    "for p in plt.gca().patches:\n",
    "    plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + 0.3, p.get_height() + 50))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PRaD-tL5RKeA",
   "metadata": {
    "id": "PRaD-tL5RKeA"
   },
   "source": [
    "## 2ï¸âƒ£ Sentiment Classification Using TextBlob\n",
    "\n",
    "In this section, we used the **TextBlob** library to automatically classify the sentiment of each customer review based on its textual content.\n",
    "\n",
    "TextBlob uses a built-in sentiment lexicon to compute a **polarity score** for each piece of text. The polarity ranges from -1 (very negative) to +1 (very positive).\n",
    "\n",
    "We defined the following thresholds for classification:\n",
    "\n",
    "- Polarity **> 0.05** â†’ **Positive**\n",
    "- Polarity **< -0.05** â†’ **Negative**\n",
    "- Otherwise â†’ **Neutral**\n",
    "\n",
    "We applied this function to the `review_text` column and stored the results in a new column named `textblob_sentiment`.\n",
    "\n",
    "This approach provides a **rule-based sentiment classification** that is independent of the star ratings, allowing us to later compare it with the manual labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fhrex8Fn7vxb",
   "metadata": {
    "id": "Fhrex8Fn7vxb"
   },
   "outputs": [],
   "source": [
    "# Install and import TextBlob (only once)\n",
    "# !pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Define a function to classify sentiment using TextBlob polarity\n",
    "def textblob_sentiment(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0.05:\n",
    "        return \"positive\"\n",
    "    elif polarity < -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Apply the function to the reviewText column\n",
    "df['textblob_sentiment'] = df['review_text'].astype(str).apply(textblob_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vTnxuGhZspFC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "vTnxuGhZspFC",
    "outputId": "1b9870d7-2972-410d-b33c-2f720ffbdbb5"
   },
   "outputs": [],
   "source": [
    "df['textblob_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LUgbccO1RYm4",
   "metadata": {
    "id": "LUgbccO1RYm4"
   },
   "source": [
    "## 3ï¸âƒ£ Sentiment Classification Using VADER\n",
    "\n",
    "In this step, we used the **VADER (Valence Aware Dictionary and sEntiment Reasoner)** sentiment analysis tool from the **NLTK** library to classify review sentiment directly from the text.\n",
    "\n",
    "VADER is particularly effective for short, informal text such as product reviews. It calculates a **compound score** for each sentence based on a dictionary of pre-labeled words and heuristics.\n",
    "\n",
    "The compound score ranges from -1 (most negative) to +1 (most positive). We used the following thresholds to categorize the reviews:\n",
    "\n",
    "- Compound **> 0.05** â†’ **Positive**\n",
    "- Compound **< -0.05** â†’ **Negative**\n",
    "- Between -0.05 and 0.05 â†’ **Neutral**\n",
    "\n",
    "We applied this method to the `review_text` column and stored the result in a new column called `vader_sentiment`.\n",
    "\n",
    "This method allows us to compare a second rule-based classifier with both the manual labels and the TextBlob results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dXyzqRjS3GA2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dXyzqRjS3GA2",
    "outputId": "79c3dd8f-e825-4837-c3af-6dab4099e5fa"
   },
   "outputs": [],
   "source": [
    "# Import NLTK and download the sentiment lexicon\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Import the VADER sentiment analyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to classify sentiment based on compound score\n",
    "def vader_sentiment(text):\n",
    "    compound = sid.polarity_scores(text)['compound']\n",
    "    if compound > 0.05:\n",
    "        return \"positive\"\n",
    "    elif compound < -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Apply the function to the reviewText column\n",
    "df['vader_sentiment'] = df['review_text'].astype(str).apply(vader_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xCJkSnaas_-z",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "id": "xCJkSnaas_-z",
    "outputId": "723348c2-3b6f-44e8-a159-66f25adc08b6"
   },
   "outputs": [],
   "source": [
    "df['vader_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aFtm3-Aw5rcv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "aFtm3-Aw5rcv",
    "outputId": "b1ce3901-1377-4ae4-b6aa-ba37525a6eed"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZHlfXYdwSq8k",
   "metadata": {
    "id": "ZHlfXYdwSq8k"
   },
   "source": [
    "We compared the sentiment predictions from TextBlob and VADER against manual labels using precision, recall, and F1-score. This evaluation helped assess how closely each tool reflects actual customer sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s30EtYCG6395",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s30EtYCG6395",
    "outputId": "a5ba6130-7102-474c-9b16-95697c5b87ca"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define mapping: positive = 1, negative = 0, neutral = -1\n",
    "sentiment_map = {'positive': 1, 'negative': 0, 'neutral': -1}\n",
    "\n",
    "# Apply mapping to sentiment results from TextBlob and VADER\n",
    "df['textblob_label'] = df['vader_sentiment'].map(sentiment_map)\n",
    "df['vader_label'] = df['vader_sentiment'].map(sentiment_map)\n",
    "\n",
    "# Remove rows where either method gave a neutral prediction (we can't compare these to binary labels)\n",
    "df_clean = df[(df['textblob_label'] != -1) & (df['vader_label'] != -1)]\n",
    "\n",
    "# Extract true labels and predictions\n",
    "y_true = df_clean['label']\n",
    "y_textblob = df_clean['textblob_label']\n",
    "y_vader = df_clean['vader_label']\n",
    "\n",
    "# Show classification performance for TextBlob\n",
    "print(\"ðŸ“Š TextBlob Performance:\")\n",
    "print(classification_report(y_true, y_textblob))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N8EI-JTTTbVV",
   "metadata": {
    "id": "N8EI-JTTTbVV"
   },
   "source": [
    "# **Visual Comparison of Sentiment Distributions**\n",
    "\n",
    "To better understand how each method classified the review sentiments, we visualized the distribution of sentiment categories across the three approaches:\n",
    "\n",
    "- **True Labels**: Manually derived from star ratings\n",
    "- **TextBlob Sentiment**: Rule-based classification from textual polarity\n",
    "- **VADER Sentiment**: Rule-based classification using compound sentiment scores\n",
    "\n",
    "This side-by-side comparison helped us quickly observe differences and potential biases in how each method perceives customer sentiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DMV8ZsBSEZLO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "DMV8ZsBSEZLO",
    "outputId": "047b6e48-a264-41b7-cec0-85c169c961d2"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create 3 bar plots to compare label distributions\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "#  Plot original labels (true labels)\n",
    "df['label'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    ax=axs[0],\n",
    "    title='True Labels',\n",
    "    color=['red', 'gray', 'green']  # 0=negative, 1=neutral, 2=positive\n",
    ")\n",
    "axs[0].set_xlabel(\"label\")\n",
    "axs[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot TextBlob sentiment results\n",
    "df['textblob_sentiment'].value_counts().loc[['negative', 'neutral', 'positive']].plot(\n",
    "    kind='bar',\n",
    "    ax=axs[1],\n",
    "    title='TextBlob Sentiment',\n",
    "    color=['red', 'gray', 'green']\n",
    ")\n",
    "axs[1].set_xlabel(\"textblob_sentiment\")\n",
    "axs[1].set_ylabel(\"Count\")\n",
    "\n",
    "# Plot VADER sentiment results\n",
    "df['vader_sentiment'].value_counts().loc[['negative', 'neutral', 'positive']].plot(\n",
    "    kind='bar',\n",
    "    ax=axs[2],\n",
    "    title='VADER Sentiment',\n",
    "    color=['red', 'gray', 'green']\n",
    ")\n",
    "axs[2].set_xlabel(\"vader_sentiment\")\n",
    "axs[2].set_ylabel(\"Count\")\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VzUW2k1kTiUA",
   "metadata": {
    "id": "VzUW2k1kTiUA"
   },
   "source": [
    "##  **Machine Learning: Sentiment Classification Approaches**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "In this section, we build machine learning models to classify review sentiment using three different labeling strategies.\n",
    "\n",
    "Each strategy provides a different perspective on how sentiment can be derived, allowing us to compare how the **source of the labels** affects model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Track 1: Model trained on manually labeled data\n",
    "- Uses sentiment labels created directly from the star rating (`rating` column).\n",
    "- Labels: 0 = Negative, 1 = Neutral, 2 = Positive.\n",
    "- Serves as the primary supervised learning setup.\n",
    "\n",
    "### ðŸ”¹ Track 2: Model trained using TextBlob-generated labels\n",
    "- Uses the `textblob_sentiment` column (positive/neutral/negative).\n",
    "- Allows us to see how a model learns from rule-based lexicon predictions.\n",
    "\n",
    "### ðŸ”¹ Track 3: Model trained using VADER-generated labels\n",
    "- Uses the `vader_sentiment` column.\n",
    "- Helps evaluate how another rule-based system performs as a labeling strategy for training.\n",
    "\n",
    "---\n",
    "\n",
    "By training and evaluating models on each of these label types, we can compare:\n",
    "- The reliability of each labeling method.\n",
    "- The generalization power of models based on different label sources.\n",
    "- How well rule-based vs. human-proxy labels support learning.\n",
    "\n",
    "Each track will follow the same modeling steps for consistency:\n",
    "1. Text preprocessing\n",
    "2. Feature extraction (TF-IDF)\n",
    "3. Model training (e.g., Logistic Regression)\n",
    "4. Evaluation using accuracy, precision, recall, and F1-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QHKzOdKHVod8",
   "metadata": {
    "id": "QHKzOdKHVod8"
   },
   "source": [
    "##  Machine Learning â€“ Track 1: Training on Manual Labels\n",
    "\n",
    "In this track, we trained multiple machine learning models to classify review sentiment using manually labeled data derived from star ratings. These labels represent three sentiment classes:\n",
    "- 0 â†’ Negative\n",
    "- 1 â†’ Neutral\n",
    "- 2 â†’ Positive\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Models Used\n",
    "We trained and evaluated the following classifiers:\n",
    "- Naive Bayes\n",
    "- Support Vector Machine (SVM)\n",
    "- Random Forest\n",
    "- Decision Tree\n",
    "- Logistic Regression\n",
    "\n",
    "Each model was implemented as a `Pipeline` consisting of:\n",
    "1. **TF-IDF Vectorizer**: to convert text into numerical features.\n",
    "2. **Classifier**: the selected machine learning algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Evaluation Metrics\n",
    "For each model, we calculated:\n",
    "- **Precision**\n",
    "- **Recall**\n",
    "- **F1 Score**\n",
    "\n",
    "We also visualized the **confusion matrix** for each model to understand the distribution of correct and incorrect predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vqzFSie3t2tx",
   "metadata": {
    "id": "vqzFSie3t2tx"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wbmNC3zEtqu4",
   "metadata": {
    "id": "wbmNC3zEtqu4"
   },
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X_manual = df['clean_review']\n",
    "y_manual = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uPpV-UMLtrZn",
   "metadata": {
    "id": "uPpV-UMLtrZn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_manual, X_test_manual, y_train_manual, y_test_manual = train_test_split(\n",
    "    X_manual, y_manual, test_size=0.2, random_state=42, stratify=y_manual\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nEMhj-T_Q70h",
   "metadata": {
    "id": "nEMhj-T_Q70h"
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"Naive Bayes\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])),\n",
    "    (\"Logistic Regression\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LogisticRegression(class_weight='balanced', max_iter=1000, solver='liblinear', random_state=42))\n",
    "    ])),\n",
    "    (\"SVM\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LinearSVC(class_weight='balanced', max_iter=1000))\n",
    "    ])),\n",
    "    (\"Decision Tree\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', DecisionTreeClassifier(random_state=42))\n",
    "    ])),\n",
    "    (\"Random Forest\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', RandomForestClassifier(n_estimators=20, random_state=42))\n",
    "    ]))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EbYdvowptrf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbYdvowptrf2",
    "outputId": "bd89ef09-3e7a-4602-e3e0-42a5c2269a48"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    classifier.fit(X_train_manual, y_train_manual)\n",
    "    predictions = classifier.predict(X_test_manual)\n",
    "\n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    print(\" Precision:\", precision_score(y_test_manual, predictions, average='macro'))\n",
    "    print(\" Recall:   \", recall_score(y_test_manual, predictions, average='macro'))\n",
    "    print(\" F1 Score: \", f1_score(y_test_manual, predictions, average='macro'))\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3SGVjDci1Bm",
   "metadata": {
    "id": "m3SGVjDci1Bm"
   },
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    preds = classifier.predict(X_test_manual)\n",
    "    results.append((name, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jj_cBgHCRV4l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jj_cBgHCRV4l",
    "outputId": "85c3954d-cea9-48c3-e027-8c00c1353477"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name, preds in results:\n",
    "    cm = confusion_matrix(y_test_manual, preds)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for {name}\")\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gwt1APh_urel",
   "metadata": {
    "id": "gwt1APh_urel"
   },
   "source": [
    "\n",
    "After training and evaluating five different models on the manually labeled dataset, we observed the following:\n",
    "\n",
    "- **Logistic Regression** achieved the highest **F1 Score (0.64)**, making it the most balanced model in terms of both precision and recall. It also showed the best generalization among all models.\n",
    "  \n",
    "- **SVM** performed reasonably well with an F1 score of **0.71**, slightly below logistic regression, but still showed strong performance, especially in recall.\n",
    "\n",
    "- **Naive Bayes** had high precision (**0.69**) and recall (**0.73**), meaning it was too conservative in predicting positive classes and missed many true positives.\n",
    "\n",
    "- **Decision Tree** and **Random Forest** performed the worst overall in terms of F1 Score (both below 0.56), likely due to overfitting on the small training sample or being too sensitive to noise.\n",
    "\n",
    "**Overall**, models based on linear decision boundaries (Logistic Regression and SVM) performed more consistently than tree-based or probabilistic models in this track.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hn7ew1Q5vRGf",
   "metadata": {
    "id": "hn7ew1Q5vRGf"
   },
   "source": [
    "##  Machine Learning â€“ Track 2: Training on TextBlob Sentiment Labels\n",
    "\n",
    "In this track, we train machine learning models using labels generated by the **TextBlob** sentiment analyzer. Each review was automatically classified by TextBlob into:\n",
    "\n",
    "- `positive` â†’ 2\n",
    "- `neutral`  â†’ 1\n",
    "- `negative` â†’ 0\n",
    "\n",
    "The goal is to assess how well models can learn sentiment when trained on rule-based (lexicon) labels, and compare performance against other labeling approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V8G5xU0wG5Y2",
   "metadata": {
    "id": "V8G5xU0wG5Y2"
   },
   "outputs": [],
   "source": [
    "# Map text sentiment to numeric labels\n",
    "sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "y_textblob = df['textblob_sentiment'].map(sentiment_map)\n",
    "X_textblob = df['clean_review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yQYz7K5KHpPB",
   "metadata": {
    "id": "yQYz7K5KHpPB"
   },
   "outputs": [],
   "source": [
    "X_train_blob, X_test_blob, y_train_blob, y_test_blob = train_test_split(\n",
    "    X_textblob, y_textblob, test_size=0.2, random_state=42, stratify=y_textblob\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6phWgo1nIKRM",
   "metadata": {
    "id": "6phWgo1nIKRM"
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"Naive Bayes\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])),\n",
    "    (\"Logistic Regression\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LogisticRegression(class_weight='balanced', max_iter=1000, solver='liblinear', random_state=42))\n",
    "    ])),\n",
    "    (\"SVM\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LinearSVC(class_weight='balanced', max_iter=1000))\n",
    "    ])),\n",
    "    (\"Decision Tree\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', DecisionTreeClassifier(random_state=42))\n",
    "    ])),\n",
    "    (\"Random Forest\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', RandomForestClassifier(n_estimators=20, random_state=42))\n",
    "    ]))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rUjLvOVIJyx-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rUjLvOVIJyx-",
    "outputId": "ee37b9e1-a119-41bd-cd5c-fce56d7fd7f9"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers:\n",
    "    print(f\"Training {name} on textblob labelsâ€¦\")\n",
    "    classifier.fit(X_train_blob, y_train_blob)\n",
    "    predictions = classifier.predict(X_test_blob)\n",
    "\n",
    "\n",
    "    print(f\"\\nFor {name}:\")\n",
    "    print(\"Precision:\", precision_score(y_test_blob, predictions, average='macro'))\n",
    "    print(\"Recall:   \", recall_score(y_test_blob, predictions, average='macro'))\n",
    "    print(\"F1 Score: \", f1_score(y_test_blob, predictions, average='macro'))\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nhWQeim3wTT1",
   "metadata": {
    "id": "nhWQeim3wTT1"
   },
   "outputs": [],
   "source": [
    "results_textblob = []\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    preds = classifier.predict(X_test_blob)\n",
    "    results_textblob.append((name, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xRvG0T5FwlKC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xRvG0T5FwlKC",
    "outputId": "c645da21-3763-452e-a84a-e123e6d4a3d9"
   },
   "outputs": [],
   "source": [
    "for name, preds in results_textblob:\n",
    "    cm = confusion_matrix(y_test_blob, preds)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for {name} (TextBlob)\")\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XOjyvi5rws_T",
   "metadata": {
    "id": "XOjyvi5rws_T"
   },
   "source": [
    "In this track, we trained the same models using sentiment labels generated by the TextBlob library. Hereâ€™s what we observed:\n",
    "\n",
    "- **Logistic Regression** again performed best with an **F1 Score of 0.69**, showing strong balance between precision and recall. This reinforces its consistency across both manual and rule-based labeling.\n",
    "\n",
    "- **Random Forest** came second with a solid F1 Score of **0.66**, showing better performance here compared to its result on manual labels. It appears to generalize better with the smoother sentiment predictions from TextBlob.\n",
    "\n",
    "- **SVM** also showed competitive results with an F1 Score of **0.67**, proving it remains a strong choice even when trained on weak labels.\n",
    "\n",
    "- **Naive Bayes**, although it had high precision (**0.77**), once again suffered from **low recall (0.43)**, meaning it failed to detect many relevant positive cases â€” similar behavior as in Track 1.\n",
    "\n",
    "- **Decision Tree** also improved from its manual-label performance, but still lagged behind the other models in overall F1 Score (**0.62**).\n",
    "\n",
    "ðŸ’¡ **Overall**, models trained on TextBlob labels showed slightly better F1 scores than with manual labels. This could be due to TextBlob generating more consistent (less noisy) training labels than human ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o6O7dD2cxcxT",
   "metadata": {
    "id": "o6O7dD2cxcxT"
   },
   "source": [
    "##  Machine Learning â€“ Track 3: Training on VADER Sentiment Labels\n",
    "\n",
    "In this final track, we train the same machine learning models using sentiment labels generated by the **VADER** sentiment analyzer.\n",
    "\n",
    "VADER classifies text into:\n",
    "- `positive` â†’ 2\n",
    "- `neutral`  â†’ 1\n",
    "- `negative` â†’ 0\n",
    "\n",
    "This track allows us to evaluate how machine learning models perform when trained on rule-based sentiment annotations from a linguistically tuned model like VADER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y_q_PPFlxgyK",
   "metadata": {
    "id": "y_q_PPFlxgyK"
   },
   "outputs": [],
   "source": [
    "# Map VADER output to numeric labels\n",
    "sentiment_map = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "y_vader = df['vader_sentiment'].map(sentiment_map)\n",
    "X_vader = df['clean_review']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6-EK_9wxg1V",
   "metadata": {
    "id": "f6-EK_9wxg1V"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_vader, X_test_vader, y_train_vader, y_test_vader = train_test_split(\n",
    "    X_vader, y_vader, test_size=0.2, random_state=42, stratify=y_vader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MJrdpdCNxg4b",
   "metadata": {
    "id": "MJrdpdCNxg4b"
   },
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (\"Naive Bayes\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', MultinomialNB())\n",
    "    ])),\n",
    "    (\"Logistic Regression\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LogisticRegression(class_weight='balanced', max_iter=1000, solver='liblinear', random_state=42))\n",
    "    ])),\n",
    "    (\"SVM\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', LinearSVC(class_weight='balanced', max_iter=1000))\n",
    "    ])),\n",
    "    (\"Decision Tree\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', DecisionTreeClassifier(random_state=42))\n",
    "    ])),\n",
    "    (\"Random Forest\", Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(max_features=5000)),\n",
    "        ('clf', RandomForestClassifier(n_estimators=20, random_state=42))\n",
    "    ]))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-HQQbUgNxhAP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-HQQbUgNxhAP",
    "outputId": "68203414-05ac-48d7-d210-7bcb5c3bae93"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, classifier in classifiers:\n",
    "  print(f\"Training {name} on VADER labelsâ€¦\")\n",
    "  classifier.fit(X_train_vader, y_train_vader)\n",
    "  predictions = classifier.predict(X_test_vader)\n",
    "\n",
    "  print(f\"\\n{name} Metrics:\")\n",
    "  print(\" Precision:\", precision_score(y_test_vader, predictions, average='macro'))\n",
    "  print(\" Recall:   \", recall_score(y_test_vader, predictions, average='macro'))\n",
    "  print(\" F1 Score: \", f1_score(y_test_vader, predictions, average='macro'))\n",
    "  print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IySvQH_y33xv",
   "metadata": {
    "id": "IySvQH_y33xv"
   },
   "outputs": [],
   "source": [
    "results_vader = []\n",
    "\n",
    "for name, classifier in classifiers:\n",
    "    preds = classifier.predict(X_test_vader)\n",
    "    results_vader.append((name, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tvuru7aU367x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tvuru7aU367x",
    "outputId": "3ff73fca-9d2f-4035-ad8f-db29d7505a93"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name, preds in results_vader:\n",
    "    cm = confusion_matrix(y_test_vader, preds)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix for {name} (VADER)\")\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IZcdnh7myeCC",
   "metadata": {
    "id": "IZcdnh7myeCC"
   },
   "source": [
    "When training the models on sentiment labels generated by VADER, we observed improved performance across most classifiers compared to Track 1 and Track 2.\n",
    "\n",
    "- **Logistic Regression** achieved the highest F1 Score (**0.71**), confirming its strength across all tracks. It also had the best recall and balanced performance, making it the most reliable model overall.\n",
    "\n",
    "- **SVM** came in second with an F1 Score of **0.70**, showing excellent generalization and stability. It also had a strong balance between precision and recall.\n",
    "\n",
    "- **Random Forest** improved significantly compared to previous tracks with an F1 Score of **0.65**, indicating it benefits from the structured sentiment provided by VADER.\n",
    "\n",
    "- **Decision Tree** also reached its best performance so far (**F1 = 0.63**), but it still lagged slightly behind the ensemble and linear models.\n",
    "\n",
    "- **Naive Bayes**, despite having decent precision (**0.73**), had the lowest F1 Score (**0.45**) due to its poor recall, consistent with previous tracks.\n",
    "\n",
    "ðŸ’¡ Overall, **Track 3 produced the strongest results**, likely because VADER's lexicon and scoring method provided smoother and more consistent labels, which helped the models learn better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BlJvIHiVWtJF",
   "metadata": {
    "id": "BlJvIHiVWtJF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "tYSnGrFUy8T6",
   "metadata": {
    "id": "tYSnGrFUy8T6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zVzMjeteyt1_",
   "metadata": {
    "id": "zVzMjeteyt1_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# collecting the data for (Model, Precision, Recall, F1 Score, Track)\n",
    "data = [\n",
    "    # Manual\n",
    "    (\"Naive Bayes\", 0.712, 0.507, 0.518, \"Manual\"),\n",
    "    (\"Logistic Regression\", 0.659, 0.673, 0.666, \"Manual\"),\n",
    "    (\"SVM\", 0.658, 0.671, 0.664, \"Manual\"),\n",
    "    (\"Decision Tree\", 0.580, 0.573, 0.576, \"Manual\"),\n",
    "    (\"Random Forest\", 0.699, 0.585, 0.595, \"Manual\"),\n",
    "\n",
    "    # TextBlob\n",
    "    (\"Naive Bayes\", 0.771, 0.453, 0.476, \"TextBlob\"),\n",
    "    (\"Logistic Regression\", 0.687, 0.713, 0.698, \"TextBlob\"),\n",
    "    (\"SVM\", 0.683, 0.710, 0.694, \"TextBlob\"),\n",
    "    (\"Decision Tree\", 0.648, 0.644, 0.646, \"TextBlob\"),\n",
    "    (\"Random Forest\", 0.755, 0.659, 0.694, \"TextBlob\"),\n",
    "\n",
    "    # VADER\n",
    "    (\"Naive Bayes\", 0.738, 0.455, 0.485, \"VADER\"),\n",
    "    (\"Logistic Regression\", 0.715, 0.751, 0.731, \"VADER\"),\n",
    "    (\"SVM\", 0.710, 0.749, 0.727, \"VADER\"),\n",
    "    (\"Decision Tree\", 0.651, 0.642, 0.646, \"VADER\"),\n",
    "    (\"Random Forest\", 0.761, 0.651, 0.693, \"VADER\")\n",
    "]\n",
    "\n",
    "# convert to DataFrame\n",
    "df_results = pd.DataFrame(data, columns=[\"Model\", \"Precision\", \"Recall\", \"F1 Score\", \"Track\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gM99neiSzNK9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "gM99neiSzNK9",
    "outputId": "fcfa8c3d-8a7c-4a26-b3c8-d4e8cca9ee01"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_results, x=\"Model\", y=\"F1 Score\", hue=\"Track\")\n",
    "\n",
    "plt.title(\"Model Comparison Across Tracks (F1 Score)\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"Label Source\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A_ZL98Mo0Bn4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "A_ZL98Mo0Bn4",
    "outputId": "be1d76f6-1d58-477f-c3d3-321fe9daa1b8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_results, x=\"Model\", y=\"Precision\", hue=\"Track\")\n",
    "plt.title(\"Model Comparison Across Tracks (Precision)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ociUGg50Bua",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "7ociUGg50Bua",
    "outputId": "947522b6-4456-47ab-8497-97124255bd68"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=df_results, x=\"Model\", y=\"Recall\", hue=\"Track\")\n",
    "plt.title(\"Model Comparison Across Tracks (Recall)\")\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3-CM-Rm2rXE",
   "metadata": {
    "id": "c3-CM-Rm2rXE"
   },
   "source": [
    "# ***Model Evaluation Summary ***\n",
    "\n",
    "**Definitions:**\n",
    "\n",
    "*   **Precision:**\n",
    "*   **Recall:**\n",
    "*   **F1 Score:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KI5oAQocd7YJ",
   "metadata": {
    "id": "KI5oAQocd7YJ"
   },
   "source": [
    "## Model Optimization Using GridSearchCV\n",
    "\n",
    "To further improve the model performance, we apply `GridSearchCV` to perform hyperparameter tuning on the best-performing model: **Logistic Regression trained on VADER sentiment labels**.\n",
    "\n",
    "This step allows us to explore different parameter combinations and select the one that achieves the best average F1-score during cross-validation.\n",
    "\n",
    "The optimization is performed using the training data from the VADER-based sentiment classification track.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jUlEMvYN57X4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUlEMvYN57X4",
    "outputId": "aa9c027f-2e33-40de-a0f6-8747e43e462b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define pipeline for logistic regression with TF-IDF\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [1000, 3000, 5000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__class_weight': [None, 'balanced'],\n",
    "    'clf__solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search using VADER-labeled training data\n",
    "grid_search.fit(X_train_vader, y_train_vader)\n",
    "\n",
    "# Show best parameters and performance\n",
    "print(\"\\n--- GridSearchCV Results ---\")\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "print(\"\\nBest score (average F1 score on the cross-validation folds):\")\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# Save the best model for predictions\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G9x-FKa2hP9u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "id": "G9x-FKa2hP9u",
    "outputId": "946191f2-77fa-4394-e2ee-ba7fcb0c1ec6"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict using the best model from GridSearchCV\n",
    "y_pred = best_model.predict(X_test_vader)\n",
    "\n",
    "# Classification report: Precision, Recall, F1 for each class\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_vader, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test_vader, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"Confusion Matrix - Best Model (VADER Track)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yg3EvhCy26zy",
   "metadata": {
    "id": "yg3EvhCy26zy"
   },
   "source": [
    "## Model Optimization Summary â€“ VADER Track (Logistic Regression)\n",
    "\n",
    "After performing hyperparameter tuning using `GridSearchCV`, the performance of the Logistic Regression model improved significantly.\n",
    "\n",
    "\n",
    "- The model performs very well in identifying positive reviews.\n",
    "- Some confusion remains between neutral and positive labels.\n",
    "- Most errors come from misclassifying neutral and negative as positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S3K_0Oj922n1",
   "metadata": {
    "id": "S3K_0Oj922n1"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anSGXsfZ1_fX",
   "metadata": {
    "id": "anSGXsfZ1_fX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06dc55e5716d43eba7081805113b336f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "07c1bbe359e94a39b8e3d522ab4b5246": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20d28c4c8afc4c4eba33f6fe31ceca86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_229c619f940141609e3b9733f1e710e4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_685fb04a9cbf483aa75b4335294a68f8",
      "value": "â€‡701528/701528â€‡[07:09&lt;00:00,â€‡2624.53it/s]"
     }
    },
    "229c619f940141609e3b9733f1e710e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e3a0f12973c4c6898c81029adcea0fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5295749466884f54bba79ce70c5d8fda": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "5a41c4d2b07547ec9b5d3c68f09b42d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cada4b069894cc6a6babe2498982617",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7f28e4f129fc4bfea5bf00a650caa6dd",
      "value": "Generatingâ€‡fullâ€‡split:â€‡"
     }
    },
    "5e16775bb94e4a9cbf2443ef07f88d34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "685fb04a9cbf483aa75b4335294a68f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "696b164107a8493e996eb148e2a602b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a41c4d2b07547ec9b5d3c68f09b42d6",
       "IPY_MODEL_8ce782328a074c0d85b81ea4e82f5afc",
       "IPY_MODEL_bd5f9b1f46234f47bb9d819e099b585c"
      ],
      "layout": "IPY_MODEL_07c1bbe359e94a39b8e3d522ab4b5246"
     }
    },
    "7b4228cafa9f4c189291508db65461f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3beacc9044f4fe8962b2e07b80587b3",
      "max": 701528,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b2eb1efb952541c9827a75244e4ae4fc",
      "value": 701528
     }
    },
    "7cada4b069894cc6a6babe2498982617": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f28e4f129fc4bfea5bf00a650caa6dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fe13dcd1c4e4167926afeaf5765c770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9a76d86096e49ccbd1e82a1534642be",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5e16775bb94e4a9cbf2443ef07f88d34",
      "value": "100%"
     }
    },
    "8ce782328a074c0d85b81ea4e82f5afc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5295749466884f54bba79ce70c5d8fda",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_06dc55e5716d43eba7081805113b336f",
      "value": 1
     }
    },
    "935942bd40e44057865e004e1a94f198": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2eb1efb952541c9827a75244e4ae4fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b3beacc9044f4fe8962b2e07b80587b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9a76d86096e49ccbd1e82a1534642be": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bd5f9b1f46234f47bb9d819e099b585c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c90ebda11d8b4a0bbb96936e5b02a101",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_935942bd40e44057865e004e1a94f198",
      "value": "â€‡701528/0â€‡[00:18&lt;00:00,â€‡50590.55â€‡examples/s]"
     }
    },
    "c90ebda11d8b4a0bbb96936e5b02a101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c98e8dcc141344afa4aac32e4796299c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7fe13dcd1c4e4167926afeaf5765c770",
       "IPY_MODEL_7b4228cafa9f4c189291508db65461f7",
       "IPY_MODEL_20d28c4c8afc4c4eba33f6fe31ceca86"
      ],
      "layout": "IPY_MODEL_4e3a0f12973c4c6898c81029adcea0fd"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
